{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXwj2LfUGGeg0PEaLxP0/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4ST4ROTH/Assignment/blob/main/NLP_A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "file_path = '/content/sample-text-file.txt'\n",
        "text = load_text(file_path)\n"
      ],
      "metadata": {
        "id": "yL4qaKDQERWe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the text\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize into sentences and words\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word_tokenize(sentence) for sentence in sentences]\n",
        "    # Remove stopwords and perform stemming\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    processed_words = [[stemmer.stem(word) for word in sentence if word not in stop_words] for sentence in words]\n",
        "    return processed_words\n",
        "\n",
        "processed_text = preprocess_text(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbBTNOHxFLea",
        "outputId": "5198cf39-70e6-4031-f16e-d8b73f348e69"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the sentence score\n",
        "def calculate_sentence_scores(sentences, processed_text):\n",
        "    sentence_scores = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        score = 0\n",
        "        for word in sentence:\n",
        "            for processed_sentence in processed_text:\n",
        "                if word in processed_sentence:\n",
        "                    score += 1\n",
        "        sentence_scores.append((i, score))\n",
        "    return sentence_scores\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "sentence_scores = calculate_sentence_scores(sentences, processed_text)\n"
      ],
      "metadata": {
        "id": "kBvDqVCrFO7t"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select the top n sentence with the highest score\n",
        "def generate_summary(sentences, sentence_scores, n=3):\n",
        "    sorted_scores = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "    top_sentences = [sentences[idx] for idx, _ in sorted_scores[:n]]\n",
        "    summary = ' '.join(top_sentences)\n",
        "    return summary\n",
        "\n",
        "summary = generate_summary(sentences, sentence_scores, n=3)\n",
        "print(\"Summary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA27kQToFVKC",
        "outputId": "430e18b2-fac7-4e46-b7ec-fafd33f96da6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, \n",
            "quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_RkZMKYLFljy"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}